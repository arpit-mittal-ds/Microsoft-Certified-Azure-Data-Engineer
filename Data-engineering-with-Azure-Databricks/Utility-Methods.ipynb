{"cells":[{"cell_type":"code","source":["\n%python\n# ****************************************************************************\n# Utility method to count & print the number of records in each partition.\n# ****************************************************************************\n\ndef printRecordsPerPartition(df):\n  def countInPartition(iterator): yield __builtin__.sum(1 for _ in iterator)\n  results = (df.rdd                   # Convert to an RDD\n    .mapPartitions(countInPartition)  # For each partition, count\n    .collect()                        # Return the counts to the driver\n  )\n  \n  print(\"Per-Partition Counts\")\n  i = 0\n  for result in results: \n    i = i + 1\n    print(\"#{}: {:,}\".format(i, result))\n  \n# ****************************************************************************\n# Utility to count the number of files in and size of a directory\n# ****************************************************************************\n\ndef computeFileStats(path):\n  bytes = 0\n  count = 0\n\n  files = dbutils.fs.ls(path)\n  \n  while (len(files) > 0):\n    fileInfo = files.pop(0)\n    if (fileInfo.isDir() == False):               # isDir() is a method on the fileInfo object\n      count += 1\n      bytes += fileInfo.size                      # size is a parameter on the fileInfo object\n    else:\n      files.extend(dbutils.fs.ls(fileInfo.path))  # append multiple object to files\n      \n  return (count, bytes)\n\n# ****************************************************************************\n# Utility method to cache a table with a specific name\n# ****************************************************************************\n\ndef cacheAs(df, name, level):\n  from pyspark.sql.utils import AnalysisException\n  print(\"WARNING: The PySpark API currently does not allow specification of the storage level - using MEMORY-ONLY\")\n  \n  try: spark.catalog.uncacheTable(name)\n  except AnalysisException: None\n  \n  df.createOrReplaceTempView(name)\n  spark.catalog.cacheTable(name)\n  #spark.catalog.cacheTable(name, level)\n  return df\n\n\n# ****************************************************************************\n# Simplified benchmark of count()\n# ****************************************************************************\n\ndef benchmarkCount(func):\n  import time\n  start = float(time.time() * 1000)                    # Start the clock\n  df = func()\n  total = df.count()                                   # Count the records\n  duration = float(time.time() * 1000) - start         # Stop the clock\n  return (df, total, duration)\n\n\n# ****************************************************************************\n# Utility method to wait until the stream is read\n# ****************************************************************************\n\ndef untilStreamIsReady(name):\n  queries = list(filter(lambda query: query.name == name, spark.streams.active))\n\n  if len(queries) == 0:\n    print(\"The stream is not active.\")\n\n  else:\n    while (queries[0].isActive and len(queries[0].recentProgress) == 0):\n      pass # wait until there is any type of progress\n\n    if queries[0].isActive:\n      print(\"The stream is active and ready.\")\n    else:\n      print(\"The stream is not active.\")\n\nNone"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ea2b0ab-0b6b-423a-9a62-fc775e0adfc3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\n// ****************************************************************************\n// Utility method to count & print the number of records in each partition.\n// ****************************************************************************\n\ndef printRecordsPerPartition(df:org.apache.spark.sql.Dataset[Row]):Unit = {\n  // import org.apache.spark.sql.functions._\n  val results = df.rdd                                   // Convert to an RDD\n    .mapPartitions(it => Array(it.size).iterator, true)  // For each partition, count\n    .collect()                                           // Return the counts to the driver\n\n  println(\"Per-Partition Counts\")\n  var i = 0\n  for (r <- results) {\n    i = i +1\n    println(\"#%s: %,d\".format(i,r))\n  }\n}\n\n// ****************************************************************************\n// Utility to count the number of files in and size of a directory\n// ****************************************************************************\n\ndef computeFileStats(path:String):(Long,Long) = {\n  var bytes = 0L\n  var count = 0L\n\n  import scala.collection.mutable.ArrayBuffer\n  var files=ArrayBuffer(dbutils.fs.ls(path):_ *)\n\n  while (files.isEmpty == false) {\n    val fileInfo = files.remove(0)\n    if (fileInfo.isDir == false) {\n      count += 1\n      bytes += fileInfo.size\n    } else {\n      files.append(dbutils.fs.ls(fileInfo.path):_ *)\n    }\n  }\n  (count, bytes)\n}\n\n// ****************************************************************************\n// Utility method to cache a table with a specific name\n// ****************************************************************************\n\ndef cacheAs(df:org.apache.spark.sql.DataFrame, name:String, level:org.apache.spark.storage.StorageLevel):org.apache.spark.sql.DataFrame = {\n  try spark.catalog.uncacheTable(name)\n  catch { case _: org.apache.spark.sql.AnalysisException => () }\n  \n  df.createOrReplaceTempView(name)\n  spark.catalog.cacheTable(name, level)\n  return df\n}\n\n// ****************************************************************************\n// Simplified benchmark of count()\n// ****************************************************************************\n\ndef benchmarkCount(func:() => org.apache.spark.sql.DataFrame):(org.apache.spark.sql.DataFrame, Long, Long) = {\n  val start = System.currentTimeMillis            // Start the clock\n  val df = func()                                 // Get our lambda\n  val total = df.count()                          // Count the records\n  val duration = System.currentTimeMillis - start // Stop the clock\n  (df, total, duration)\n}\n\n// ****************************************************************************\n// Benchmarking and cache tracking tool\n// ****************************************************************************\n\ncase class JobResults[T](runtime:Long, duration:Long, cacheSize:Long, maxCacheBefore:Long, remCacheBefore:Long, maxCacheAfter:Long, remCacheAfter:Long, result:T) {\n  def printTime():Unit = {\n    if (runtime < 1000)                 println(f\"Runtime:  ${runtime}%,d ms\")\n    else if (runtime < 60 * 1000)       println(f\"Runtime:  ${runtime/1000.0}%,.2f sec\")\n    else if (runtime < 60 * 60 * 1000)  println(f\"Runtime:  ${runtime/1000.0/60.0}%,.2f min\")\n    else                                println(f\"Runtime:  ${runtime/1000.0/60.0/60.0}%,.2f hr\")\n    \n    if (duration < 1000)                println(f\"All Jobs: ${duration}%,d ms\")\n    else if (duration < 60 * 1000)      println(f\"All Jobs: ${duration/1000.0}%,.2f sec\")\n    else if (duration < 60 * 60 * 1000) println(f\"All Jobs: ${duration/1000.0/60.0}%,.2f min\")\n    else                                println(f\"Job Dur: ${duration/1000.0/60.0/60.0}%,.2f hr\")\n  }\n  def printCache():Unit = {\n    if (Math.abs(cacheSize) < 1024)                    println(f\"Cached:   ${cacheSize}%,d bytes\")\n    else if (Math.abs(cacheSize) < 1024 * 1024)        println(f\"Cached:   ${cacheSize/1024.0}%,.3f KB\")\n    else if (Math.abs(cacheSize) < 1024 * 1024 * 1024) println(f\"Cached:   ${cacheSize/1024.0/1024.0}%,.3f MB\")\n    else                                               println(f\"Cached:   ${cacheSize/1024.0/1024.0/1024.0}%,.3f GB\")\n    \n    println(f\"Before:   ${remCacheBefore / 1024.0 / 1024.0}%,.3f / ${maxCacheBefore / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheBefore/maxCacheBefore}%.2f%%\")\n    println(f\"After:    ${remCacheAfter / 1024.0 / 1024.0}%,.3f / ${maxCacheAfter / 1024.0 / 1024.0}%,.3f MB / ${100.0*remCacheAfter/maxCacheAfter}%.2f%%\")\n  }\n  def print():Unit = {\n    printTime()\n    printCache()\n  }\n}\n\ncase class Node(driver:Boolean, executor:Boolean, address:String, maximum:Long, available:Long) {\n  def this(address:String, maximum:Long, available:Long) = this(address.contains(\"-\"), !address.contains(\"-\"), address, maximum, available)\n}\n\nclass Tracker() extends org.apache.spark.scheduler.SparkListener() {\n  \n  sc.addSparkListener(this)\n  \n  val jobStarts = scala.collection.mutable.Map[Int,Long]()\n  val jobEnds = scala.collection.mutable.Map[Int,Long]()\n  \n  def track[T](func:() => T):JobResults[T] = {\n    jobEnds.clear()\n    jobStarts.clear()\n\n    val executorsBefore = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n    val maxCacheBefore = executorsBefore.map(_.maximum).sum\n    val remCacheBefore = executorsBefore.map(_.available).sum\n    \n    val start = System.currentTimeMillis()\n    val result = func()\n    val runtime = System.currentTimeMillis() - start\n    \n    Thread.sleep(1000) // give it a second to catch up\n\n    val executorsAfter = sc.getExecutorMemoryStatus.map(x => new Node(x._1, x._2._1, x._2._2)).filter(_.executor)\n    val maxCacheAfter = executorsAfter.map(_.maximum).sum\n    val remCacheAfter = executorsAfter.map(_.available).sum\n\n    var duration = 0L\n    \n    for ((jobId, startAt) <- jobStarts) {\n      assert(jobEnds.keySet.exists(_ == jobId), s\"A conclusion for Job ID $jobId was not found.\") \n      duration += jobEnds(jobId) - startAt\n    }\n    JobResults(runtime, duration, remCacheBefore-remCacheAfter, maxCacheBefore, remCacheBefore, maxCacheAfter, remCacheAfter, result)\n  }\n  override def onJobStart(jobStart: org.apache.spark.scheduler.SparkListenerJobStart):Unit = jobStarts.put(jobStart.jobId, jobStart.time)\n  override def onJobEnd(jobEnd: org.apache.spark.scheduler.SparkListenerJobEnd): Unit = jobEnds.put(jobEnd.jobId, jobEnd.time)\n}\n\nval tracker = new Tracker()\n\n\n// ****************************************************************************\n// Utility method to wait until the stream is read\n// ****************************************************************************\n\ndef untilStreamIsReady(name:String):Unit = {\n  val queries = spark.streams.active.filter(_.name == name)\n\n  if (queries.length == 0) {\n    println(\"The stream is not active.\")\n  } else {\n    while (queries(0).isActive && queries(0).recentProgress.length == 0) {\n      // wait until there is any type of progress\n    }\n\n    if (queries(0).isActive) {\n      println(\"The stream is active and ready.\")\n    } else {\n      println(\"The stream is not active.\")\n    }\n  }\n}\n\ndisplayHTML(\"\"\"\n<div>Declared various utility methods:</div>\n<li>Declared <b style=\"color:green\">printRecordsPerPartition(<i>df:DataFrame</i>)</b> for diagnostics</li>\n<li>Declared <b style=\"color:green\">computeFileStats(<i>path:String</i>)</b> returns <b style=\"color:green\">(count:Long, bytes:Long)</b> for diagnostics</li>\n<li>Declared <b style=\"color:green\">tracker</b> for benchmarking</li>\n<li>Declared <b style=\"color:green\">cacheAs(<i>df:DataFrame, name:String, level:StorageLevel</i>)</b> for better debugging</li>\n<li>Declared <b style=\"color:green\">benchmarkCount(<i>lambda:DataFrame</i>)</b> returns <b style=\"color:green\">(df:DataFrame, total:Long, duration:Long)</b> for diagnostics</li>\n<li>Declared <b style=\"color:green\">untilStreamIsReady(<i>name:String</i>)</b> to control workflow</li>\n<br/>\n<div>All done!</div>\n\"\"\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"697c41c7-5f8d-43ac-80e9-1bb1e88c55fc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Utility-Methods","dashboards":[],"language":"python","widgets":{},"notebookOrigID":42263567104176}},"nbformat":4,"nbformat_minor":0}
